[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lake Erie Wind and Wave Data Analysis and Visulization",
    "section": "",
    "text": "This project is a part of my data analysis for my research assistant work. It includes data gathering from multiple sources, visualizations, and regression models. In the analysis tab on the left, you can browse through the process and have a look at the final products for both wind and wave data. The final step includes a regression analysis that is built on all the previous steps."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Lake Erie Wind and Wave Data Analysis and Visulization",
    "section": "",
    "text": "This project is a part of my data analysis for my research assistant work. It includes data gathering from multiple sources, visualizations, and regression models. In the analysis tab on the left, you can browse through the process and have a look at the final products for both wind and wave data. The final step includes a regression analysis that is built on all the previous steps."
  },
  {
    "objectID": "index.html#find-out-more",
    "href": "index.html#find-out-more",
    "title": "Lake Erie Wind and Wave Data Analysis and Visulization",
    "section": "Find out more",
    "text": "Find out more\nThe code for this repository is hosted on GitHub page: https://github.com/junyi2022/musa-550-quarto."
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive maps produced using Folium."
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Interactive Maps with Folium",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Interactive Maps with Folium",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/2-static-images.html",
    "href": "analysis/2-static-images.html",
    "title": "Showing static visualizations",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and demonstrates how to generate static visualizations with matplotlib, pandas, and seaborn.\nStart by importing the packages we need:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nLoad the “Palmer penguins” dataset from week 2:\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/penguins.csv\")\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007"
  },
  {
    "objectID": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "href": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "title": "Showing static visualizations",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs. bill length, colored by the penguin species\n\n\nUsing matplotlib\n\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group_df in penguins.groupby(\"species\"):\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group_df[\"flipper_length_mm\"],\n        group_df[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n        zorder=10\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True);\n\n\n\n\n\n\nHow about in pandas?\nDataFrames have a built-in “plot” function that can make all of the basic type of matplotlib plots!\nFirst, we need to add a new “color” column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\nNow plot!\n\n# Same as before: Start by initializing the figure and axes\nfig, myAxes = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=myAxes, # IMPORTANT: Make sure to plot on the axes object we created already!\n    zorder=10\n)\n\n# Format the axes finally\nmyAxes.set_xlabel(\"Flipper Length (mm)\")\nmyAxes.set_ylabel(\"Bill Length (mm)\")\nmyAxes.grid(True);\n\n\n\n\nNote: no easy way to get legend added to the plot in this case…\n\n\nSeaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column…\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\",  # the x column\n    y=\"bill_length_mm\",  # the y column\n    hue=\"species\",  # the third dimension (color)\n    data=penguins,  # pass in the data\n    ax=ax,  # plot on the axes object we made\n    **style  # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc=\"best\");"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "On this about page, you might want to add more information about yourself, the project, or course. Any helpful context could go here!\nMy name is Nick Hand, the instructor for the course. You can find more information about me on my personal website.\nThis site is an example site showing how to use Quarto for the final project for MUSA 550, during fall 2023.\nAdipisicing proident minim non non dolor quis. Pariatur in ipsum aliquip magna. Qui ad aliqua nulla excepteur dolor nostrud quis nisi. Occaecat proident eiusmod in cupidatat. Elit qui laboris sit aliquip proident dolore. Officia commodo commodo in eiusmod aliqua sint cupidatat consectetur aliqua sint reprehenderit.\nOccaecat incididunt esse et elit adipisicing sit est cupidatat consequat. Incididunt exercitation amet dolor non sit anim veniam veniam sint velit. Labore irure reprehenderit ut esse. Minim quis commodo nisi voluptate."
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot."
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap"
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nThis section includes technical analysis done using Jupyter notebooks. Each sub-section highlights different components of analysis and visualizations in this project.\nYou can also see those as steps to processing the data.\n1. Wind data cleaning and visualizations\n2. Web scraping to gather wave data\n3. Wave data cleaning and visualizations\n4. Wind data and wave data correlation analysis using regression models"
  },
  {
    "objectID": "analysis/wind-data-visulize.html",
    "href": "analysis/wind-data-visulize.html",
    "title": "Wind Data Visualization",
    "section": "",
    "text": "Wind Data Source: ERA5 climate data from Climate Data Store\nThe wind data in this project comes from the ‘ERA5 monthly averaged data on single levels from 1940 to present’ dataset, specifically the 10m u-component of wind and 10m v-component of wind.\nThe time period will be 2022 yearly data, and the area of interest will be Lake Erie.\n\n\n\n\n\n\nNote\n\n\n\nEven though during downloading process, it says the data is in a 10 meters resolution, the actual resolution of the .nc file is much larger than that.\n\n\n\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport xarray as xr\nfrom matplotlib import pyplot as plt\nimport datashader as ds\nimport imageio\n\n\nStep 1: Read the .nc file downloaded from the ERA5 website and calculate wind strength\nRead .nc file downloaded from Climate Data Store\n\nwindnc = xr.open_dataset('./data/ERA5climate4.nc')  \n\nExtract latitude and longitude data of each point\n\nlat = windnc.latitude\nlon = windnc.longitude\n\nExtract wind uv data\n\nwind_u = windnc.u10\nwind_v = windnc.v10\n\nCalculate wind strength (m/s) based on wind uv data\n\nWS = np.sqrt(wind_u**2 + wind_v**2)\n\n\n\nStep 2: plot wind field map for each month in 2022 and generate a gif\nRead the Lake Erie shapefile extracted from USGS National Hydrography Dataset and change it to the same projection as wind data (CRS 4326)\n\nlake = gpd.read_file(\"./data/LakeErie\")\n\n\nlake4326 = lake.to_crs(epsg=4326)\n\nDefine a function to collect the plot for each month\nThe plot contains three components: the wind field map (array array), the wind strength map (color), and Lake Erie location (shape).\nThe wind field map is generated through vector calculation.\nThe wind strength map is generated through each point’s relationship to its neighbors.\n\n# Have a list of month names for the function to use\nmonthlist = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n\n\ndef plot_wind_by_month(month):\n    \"\"\"Plot wind field map for each month in 2022\"\"\"\n    fig, ax = plt.subplots(figsize=(10,6))\n\n    # Wind strength map\n    cf = plt.contourf(lon,lat,WS[month,:,:], levels=15, cmap='rainbow', alpha=0.8)\n    \n    # lake Erie shape\n    lake4326.plot(ax=plt.gca(), edgecolor='#25408F', facecolor='none', linewidth=0.5)\n    \n    # Wind field map with labels\n    Q = plt.quiver(lon,lat,wind_u[month,:,:],wind_v[month,:,:], scale_units='xy', scale=4, width=0.0014, color='white')\n    qk = plt.quiverkey(Q, \n                  1, 1.02, \n                   1,str(1)+' m/s',   \n                   labelpos='E',                \n                   coordinates='axes',\n                   color='black'\n                   )\n\n    # style adjustments\n    cb = plt.colorbar(cf, fraction=0.0235, pad=0.03)\n    cb.set_label('m/s',  fontsize=11)\n    plt.xlabel('Longitude')\n    plt.ylabel('Latitude')\n    plt.title(f'Wind Field Map of {monthlist[month]}')\n\n    # prepare for imageio\n    fig.canvas.draw()\n    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype=\"uint8\")\n    image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n    \n    return image\n\nUse the function above to see the wind strenghth for each month and generate a gif for the wind visualization\n\nimgs = []\nfor month in range(12):\n    img = plot_wind_by_month(month)\n    imgs.append(img)\n\n# Combing the images for each month into a single GIF\nimageio.mimsave(\"wind-field.gif\", imgs, duration=1000)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe gif generated above does not have infinite loop, so Onlinegiftools can help change that property\nThe final visualization is below:\n\n\n\nStep 3: Analyze the result\nResult:\nThe gif works smoothly and clearly displays the wind change within 2022. The prevailing wind for Lake Erie is southwest, which is because of the typical seiche pattern. Because all the vectors of the wind field map are on the same scale, we can see that Lake Erie has severely larger wind in the winter.\nEvent:\nThe dominant southwest wind usually causes much damage to residents near the coastline in winter. Historic winter storms dumped 50 inches of snow per year in the region. In December 2022, blizzards and seiche killed 50 people, including 27 in Buffalo, and created a catastrophic blanket of ice across the city.\nCompare:\nThe gif result can reflect the event above. In November and December, the wind field arrays are much longer than the arrays in other months."
  },
  {
    "objectID": "analysis/web-scraping-wave-data.html",
    "href": "analysis/web-scraping-wave-data.html",
    "title": "Web Scraping of Wave Height Data from Buoys",
    "section": "",
    "text": "Wave Data Source: WIS Data Portal\nThe wave data in this project comes from the buoys’ on Lake Erie. The data gathering process uses web scraping because there are too many bouys to be downloaded manully.\nThe time period will be 2022 yearly data, and the area of interest is the part of Lake Erie in New York State’s boundary (highlighted in yellow).\n\n\nimport requests\n# Import the webdriver from selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\n\n\nfrom time import sleep\n\n\nStep 1: Set up driver\n\ndriver = webdriver.Chrome()\n\nThis website takes a long time to load, so here we will stop for 10 seconds before continue the following steps\n\nurl = \"https://wisportal.erdc.dren.mil/#\"\ndriver.get(url)\nsleep(10)\n\n\n\nStep 2: Add wave height data of each buoy to an export group online\nBecause the buoys buttons on the website are markers generated by leaflet, the ID selectors associated with each marker are in a random numbering. Firstly, we need to have a list of all the buoy ID in the prefered sequence, which is from the south to the north.\n\n\n\n\n\n\nImportant\n\n\n\nBecause of the randomness associated with the leaflet marker ID, some of them may not be the same ID for each session (although most of them will stay the same). The ID may be slightly varied from the list below. Make sure to recheck them before reusing them.\n\n\n\nmarker_list = ['#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(3014)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2281)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2241)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2278)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2275)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2547)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2886)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2545)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2272)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2881)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(3009)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2269)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2266)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(3012)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(3010)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2263)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2876)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2872)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2543)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(3006)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2867)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(3007)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2541)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2384)']\n\nSince there are a lot of buoys, a function can help us handle the scraping process of each individual\n\ndef add_each_marker_to_export_group(marker_selector):\n    \"\"\"handle the scraping process of each marker\"\"\"\n    # click marker\n    buoy_input = driver.find_element(By.CSS_SELECTOR, marker_selector)\n    buoy_input.click()\n    sleep(1)\n    \n    # click general export button\n    general_export_selector = \"#generic_export\"\n    general_export_input = driver.find_element(By.CSS_SELECTOR, general_export_selector)\n    general_export_input.click()\n    sleep(1)\n    \n    # check wave height checkbox\n    waveheight_selector = \"#check-waveHs\"\n    waveheight_check = driver.find_element(By.CSS_SELECTOR, waveheight_selector)\n    waveheight_check.click()\n    sleep(1)\n    \n    # add to export group\n    add_to_export_group_selector = \"#ep-export-button\"\n    add_to_export_group_button = driver.find_element(By.CSS_SELECTOR, add_to_export_group_selector)\n    add_to_export_group_button.click()\n    sleep(1)\n\nApply the above function to each marker in the list\n\nfor i in range(len(marker_list)):\n    marker_selector = marker_list[i]\n    add_each_marker_to_export_group(marker_selector)\n    sleep(1)\n    \n\n\n\nStep 3: Download the export group\nAfter adding all the buoy data we need, we will download the export group summary, which will be a zip file\n\n# Go to the export group summary page\nexport_group_selector = \"#export-summary\"\nexport_group_button = driver.find_element(By.CSS_SELECTOR, export_group_selector)\nexport_group_button.click()\n\n# Download the data we need\ndownload_selector = \"#ep-download-all-button\"\ndownload_button = driver.find_element(By.CSS_SELECTOR, download_selector)\ndownload_button.click()\nsleep(10)\n\n\n\nStep 4: Close Driver\n\ndriver.close()"
  },
  {
    "objectID": "analysis/wave-visualization.html",
    "href": "analysis/wave-visualization.html",
    "title": "Wave Data Visualization",
    "section": "",
    "text": "Wave Data Source: WIS Data Portal\nThe wave data in this project comes from the web scraping page.\nThe time period will be 2022 yearly data, and the area of interest is the part of Lake Erie in New York State’s boundary.\n\nimport altair as alt\nimport numpy as np\nimport pandas as pd\nimport holoviews as hv\nimport hvplot.pandas\n# Load bokeh\nhv.extension(\"bokeh\")\nimport geopandas as gpd\n%matplotlib inline\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport panel as pn\n\n\nStep 1: Read all the downloaded data from csv\nThe zip file from the web scraping process includes multiple csv (one for each buoy)\n\nST92023_raw = pd.read_csv(\"./data/web-scarp/1-ST92023-generic_export-20231218T20_22.csv\")\nST92022_raw = pd.read_csv(\"./data/web-scarp/2-ST92022-generic_export-20231218T20_22.csv\")\nST92021_raw = pd.read_csv(\"./data/web-scarp/3-ST92021-generic_export-20231218T20_22.csv\")\nST92020_raw = pd.read_csv(\"./data/web-scarp/4-ST92020-generic_export-20231218T20_22.csv\")\nST92019_raw = pd.read_csv(\"./data/web-scarp/5-ST92019-generic_export-20231218T20_22.csv\")\nST92018_raw = pd.read_csv(\"./data/web-scarp/6-ST92018-generic_export-20231218T20_22.csv\")\nST92017_raw = pd.read_csv(\"./data/web-scarp/7-ST92017-generic_export-20231218T20_22.csv\")\nST92016_raw = pd.read_csv(\"./data/web-scarp/8-ST92016-generic_export-20231218T20_22.csv\")\nST92015_raw = pd.read_csv(\"./data/web-scarp/9-ST92015-generic_export-20231218T20_22.csv\")\nST92014_raw = pd.read_csv(\"./data/web-scarp/10-ST92014-generic_export-20231218T20_22.csv\")\nST92013_raw = pd.read_csv(\"./data/web-scarp/11-ST92013-generic_export-20231218T20_22.csv\")\nST92012_raw = pd.read_csv(\"./data/web-scarp/12-ST92012-generic_export-20231218T20_22.csv\")\nST92011_raw = pd.read_csv(\"./data/web-scarp/13-ST92011-generic_export-20231218T20_22.csv\")\nST92010_raw = pd.read_csv(\"./data/web-scarp/14-ST92010-generic_export-20231218T20_22.csv\")\nST92009_raw = pd.read_csv(\"./data/web-scarp/15-ST92009-generic_export-20231218T20_22.csv\")\nST92008_raw = pd.read_csv(\"./data/web-scarp/16-ST92008-generic_export-20231218T20_22.csv\")\nST92007_raw = pd.read_csv(\"./data/web-scarp/17-ST92007-generic_export-20231218T20_22.csv\")\nST92006_raw = pd.read_csv(\"./data/web-scarp/18-ST92006-generic_export-20231218T20_22.csv\")\nST92005_raw = pd.read_csv(\"./data/web-scarp/19-ST92005-generic_export-20231218T20_22.csv\")\nST92004_raw = pd.read_csv(\"./data/web-scarp/20-ST92004-generic_export-20231218T20_22.csv\")\nST92003_raw = pd.read_csv(\"./data/web-scarp/21-ST92003-generic_export-20231218T20_22.csv\")\nST92002_raw = pd.read_csv(\"./data/web-scarp/22-ST92002-generic_export-20231218T20_22.csv\")\nST92001_raw = pd.read_csv(\"./data/web-scarp/23-ST92001-generic_export-20231218T20_22.csv\")\nST92243_raw = pd.read_csv(\"./data/web-scarp/24-ST92243-generic_export-20231218T20_22.csv\")\n\n\n# A view of the raw data:\nST92023_raw.head()\n\n\n\n\n\n\n\n\ntime\nlatitude\nlongitude\nwaveHs\n\n\n\n\n0\n2022-01-01 00:00:00\n42.32\n-79.88\n0.132812\n\n\n1\n2022-01-01 01:00:00\n42.32\n-79.88\n0.140625\n\n\n2\n2022-01-01 02:00:00\n42.32\n-79.88\n0.140625\n\n\n3\n2022-01-01 03:00:00\n42.32\n-79.88\n0.140625\n\n\n4\n2022-01-01 04:00:00\n42.32\n-79.88\n0.132812\n\n\n\n\n\n\n\n\n# Date type for each column\nST92023_raw.dtypes\n\ntime          object\nlatitude     float64\nlongitude    float64\nwaveHs       float64\ndtype: object\n\n\n\n\nStep 2: Clean the raw data\nSince there are 24 files, a list will be helpful for later manipulations\n\nbuoy_list_raw = [\nST92023_raw, ST92022_raw, ST92021_raw, ST92020_raw, ST92019_raw,\nST92018_raw, ST92017_raw, ST92016_raw, ST92015_raw, ST92014_raw,\nST92013_raw, ST92012_raw, ST92011_raw, ST92010_raw, ST92009_raw,\nST92008_raw, ST92007_raw, ST92006_raw, ST92005_raw, ST92004_raw,\nST92003_raw, ST92002_raw, ST92001_raw, ST92243_raw\n]\n\nFirstly, clean the raw data. This process includes handling datetime object and drop null value.\n\nbuoy_list = []\nfor buoy in buoy_list_raw:\n    #Convert date type\n    buoy[\"datetime\"] = pd.to_datetime(buoy[\"time\"])\n    \n    #Drop Null value\n    buoy_clean = buoy.drop(buoy[buoy['waveHs'] &lt; 0].index)\n    \n    # Save it\n    buoy_list.append(buoy_clean)\n\n\nlen(buoy_list)\n\n24\n\n\nAnd then, change each buoy’s data to a prefered structure, which includes a column for month and a column for day of year. Each buoy will also have its station name in the column\n\nbuoy_name = [\n'ST92023', 'ST92022', 'ST92021', 'ST92020', 'ST92019',\n'ST92018', 'ST92017', 'ST92016', 'ST92015', 'ST92014',\n'ST92013', 'ST92012', 'ST92011', 'ST92010', 'ST92009',\n'ST92008', 'ST92007', 'ST92006', 'ST92005', 'ST92004',\n'ST92003', 'ST92002', 'ST92001', 'ST92243'\n]\n\n\nfor i in range(len(buoy_list)):\n    buoy = buoy_list[i]\n    \n    #Arrange by month prepare\n    buoy[\"month_int\"] = buoy[\"datetime\"].dt.month\n    buoy[\"month\"] = buoy[\"datetime\"].dt.strftime(\"%b\")\n    \n    #Arrange by date prepare\n    buoy[\"day_int\"] = buoy[\"datetime\"].dt.dayofyear\n    \n    # Add station name\n    buoy[\"station\"] = buoy_name[i]\n    \n\nComebine seperate dataframes into a single dataframe\n\nbuoy_clean_combine = pd.concat(buoy_list)\n\n\nlen(buoy_clean_combine)\n\n187680\n\n\nA view of the final dataframe\n\nbuoy_clean_combine.head()\n\n\n\n\n\n\n\n\ntime\nlatitude\nlongitude\nwaveHs\ndatetime\nmonth_int\nmonth\nday_int\nstation\n\n\n\n\n0\n2022-01-01 00:00:00\n42.32\n-79.88\n0.132812\n2022-01-01 00:00:00\n1\nJan\n1\nST92023\n\n\n1\n2022-01-01 01:00:00\n42.32\n-79.88\n0.140625\n2022-01-01 01:00:00\n1\nJan\n1\nST92023\n\n\n2\n2022-01-01 02:00:00\n42.32\n-79.88\n0.140625\n2022-01-01 02:00:00\n1\nJan\n1\nST92023\n\n\n3\n2022-01-01 03:00:00\n42.32\n-79.88\n0.140625\n2022-01-01 03:00:00\n1\nJan\n1\nST92023\n\n\n4\n2022-01-01 04:00:00\n42.32\n-79.88\n0.132812\n2022-01-01 04:00:00\n1\nJan\n1\nST92023\n\n\n\n\n\n\n\nSave the dataframe for future usage\n\nbuoy_clean_combine.to_csv(\"./data/wave2022.csv\", index=False)\n\n\n\nStep 3: Line Chart Visualization\nThe first line chart plot shows the wave height of different buoys in 2022. By dragging the widget from left to right, you can have a look at the buoys located from south to north\n\nclean_combine_chart = buoy_clean_combine.hvplot(\n    x=\"datetime\", \n    y=\"waveHs\", \n    groupby=\"station\", \n    width=900, \n    kind=\"line\", \n    widgets={'station': pn.widgets.DiscreteSlider},\n    widget_location='bottom',\n    color='#E07069',\n    line_width=0.8,\n    title='Lake Erie NYS Shoreline 2022 Wave Height',\n    xlabel=\"Date\",\n    ylabel=\"Wave Height / m\"\n)\n\nclean_combine_chart\n\n\n\n\n\n\n\n\n  \n\n\n\n\nObservations:\nThe wave height recorded by each buoy is usually turbulent between different days. However, the overall trend is that buoys encounter higher waves in winter.\nNote:\nAfter searching for hours I would conclude that Quarto does not support the hvplot widget in the notebook. If want to see the live version of the above widget, please refer to final project’s repository and open it in jupyter notebook instead. There are discussions about this issue. The conclusion is that “The kind of Jupyter interactivity that we can’t easily support directly in Quarto is that which involves a live Jupyter process for every open web browser. In order for ipywidgets.interact to work, a jupyter kernel needs to be live and running, and that is a fundamentally more involved mode of deployment.” Alternatively, there are ways to possibly solve this issue by using shinylive package which involves NodeJS and suported by Quarto website. However, NodeJS is out of the scope of this project and will not be included here.\nThe second line plot compares the southest buoy to the northest buoy in order to show the trend across the shoreline. ST92023 is the southest buoy along the NYS shoreline of Lake Erie, and ST92243 is the northest buoy along the NYS shoreline of Lake Erie, near Buffalo\n\ntwo_end = ['ST92243', 'ST92023']\nends = buoy_clean_combine.loc[buoy_clean_combine['station'].isin(two_end)]\n\n\nend_chart = ends.hvplot(\n    x=\"datetime\", \n    y=\"waveHs\", \n    by=\"station\",\n    kind=\"line\",\n    color=['#E07069', '#6989E0'],\n    line_width=0.8,\n    width=900,\n    title=\"Compare North and South End's Wave Height\",\n    xlabel=\"Date\",\n    ylabel=\"Wave Height / m\"\n)\nend_chart\n\n\n\n\n\n  \n\n\n\n\nObservation:\nThe south buoy has higher waves than the north buoy. This is probably because the prevailing wind of Lake Erie is southwest. Besides, the north buoy has a long period without data (from February to April), which is probably because the lake is frozen during that time.\n\n\nStep 4: Heat map visua\nThis step will calculate the average wave height for each month or each day in 2022 and show the results in heatmaps\n\nheight_heat = buoy_clean_combine.hvplot.heatmap(x='month_int', \n                                                y='station', \n                                                C='waveHs', \n                                                reduce_function=np.mean, \n                                                cmap='Magma', \n                                                width=900, \n                                                height=300, \n                                                colorbar=True,\n                                                title=\"Lake Erie NYS Shoreline 2022 Buoys'Wave Height by Month\")\n\nheight_heat.redim(station=\"Stations\", month_int=\"Month\")\n\n\n\n\n\n  \n\n\n\n\nObservation:\nThe wave height is higher in winter (November to January) and lower in summer (May to August). There is a small time lag for the change of wave height, which means wave height of south shoreline changes earlier than the north shoreline.\n\nheight_heat = buoy_clean_combine.hvplot.heatmap(x='day_int', \n                                                y='station', \n                                                C='waveHs', \n                                                reduce_function=np.mean, \n                                                width=900, \n                                                height=300, \n                                                colorbar=True,\n                                                cmap='Magma',\n                                                title=\"Lake Erie NYS Shoreline 2022 Buoys'Wave Height by Day\")\nheight_heat.redim(station=\"Stations\", day_int=\"Day of 2022\")\n\n\n\n\n\n  \n\n\n\n\nObservation:\nIn Febuary and March, a lot of buoys have missing data, which is very likly because of the frozen lake. In December, there was a large blizzards and seiche event, which is indicated in this plot. In a single day, the wave height was above 7 meters for almost all the buoys along the NYS shoreline of Lake Erie."
  },
  {
    "objectID": "analysis/wave-wind-analysis.html",
    "href": "analysis/wave-wind-analysis.html",
    "title": "Wind Strength and Wave Height Correlation Analysis",
    "section": "",
    "text": "Data Source:\n1. Wind data comes from ERA5 climate data from Climate Data Store The wind data in this project comes from the ‘ERA5 monthly averaged data on single levels from 1940 to present’ dataset, specifically the 10m u-component of wind and 10m v-component of wind.\nThe time period will be 2022 yearly data, and the area of interest will be Lake Erie. 2. Wave data comes from the final dataframe of the wave visualization page.\n\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport xarray as xr\nfrom matplotlib import pyplot as plt\nimport holoviews as hv\nimport hvplot.pandas\n# Load bokeh\nhv.extension(\"bokeh\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\n\nStep 1: Read the .nc file downloaded from the ERA5 website and change it to a dataframe\nNeed to use xarry to read netCDF file\n\nwindnc = xr.open_dataset('./data/ERA5land.nc') \nwinddf = windnc.to_dataframe().reset_index()\n\nThe raw data looks like this\n\nwinddf.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\ntime\nu10\nv10\n\n\n\n\n0\n-80.0\n43.400002\n2022-01-01\n1.490095\n-0.002929\n\n\n1\n-80.0\n43.400002\n2022-02-01\n1.628315\n0.594098\n\n\n2\n-80.0\n43.400002\n2022-03-01\n1.543347\n-0.034178\n\n\n3\n-80.0\n43.400002\n2022-04-01\n1.104076\n0.152090\n\n\n4\n-80.0\n43.400002\n2022-05-01\n-0.122249\n0.055608\n\n\n\n\n\n\n\n\nwinddf.dtypes\n\nlongitude           float64\nlatitude            float64\ntime         datetime64[ns]\nu10                 float32\nv10                 float32\ndtype: object\n\n\n\n\nStep 2: Calculate wind strength and change to geodataframe\nThe wind strength is based on calculation of wind u and wind v\n\nwinddf['wind_strength'] = np.sqrt(winddf['u10']**2 + winddf['v10']**2)\n\nCurrently the data is arranged by days, but we need month value. Manipulate the dataframe to get the month column\n\nwinddf[\"month_int\"] = winddf[\"time\"].dt.month\nwind_month = winddf.groupby([\"month_int\", \"longitude\", \"latitude\"])[[\"wind_strength\"]].mean().reset_index()\n\nUse the latitude and longitude column to get a geodataframe of wind data\n\nwind_month[\"geometry\"] = gpd.points_from_xy(\n   wind_month[\"longitude\"], wind_month[\"latitude\"]\n)\nwind_month_gdf = gpd.GeoDataFrame(\n    wind_month, geometry=\"geometry\", crs=\"EPSG:4326\"\n)\n\nThe geodataframe looks like this\n\nwind_month_gdf.head()\n\n\n\n\n\n\n\n\nmonth_int\nlongitude\nlatitude\nwind_strength\ngeometry\n\n\n\n\n0\n1\n-80.0\n42.000000\n1.075989\nPOINT (-80.00000 42.00000)\n\n\n1\n1\n-80.0\n42.099998\n1.489239\nPOINT (-80.00000 42.10000)\n\n\n2\n1\n-80.0\n42.200001\n1.999220\nPOINT (-80.00000 42.20000)\n\n\n3\n1\n-80.0\n42.299999\n2.441840\nPOINT (-80.00000 42.30000)\n\n\n4\n1\n-80.0\n42.400002\n2.715811\nPOINT (-80.00000 42.40000)\n\n\n\n\n\n\n\nHave a look at the location of each wind data’s point\n\nwind_month_gdf.explore(column=\"wind_strength\", \n                 tiles=\"CartoDB positron\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nStep 3: Read the wave dataframe that is cleaned in the wave data visualization page\n\nwave = pd.read_csv(\"./data/wave2022.csv\")\n\n\nwave.head()\n\n\n\n\n\n\n\n\ntime\nlatitude\nlongitude\nwaveHs\ndatetime\nmonth_int\nmonth\nday_int\nstation\n\n\n\n\n0\n2022-01-01 00:00:00\n42.32\n-79.88\n0.132812\n2022-01-01 00:00:00\n1\nJan\n1\nST92023\n\n\n1\n2022-01-01 01:00:00\n42.32\n-79.88\n0.140625\n2022-01-01 01:00:00\n1\nJan\n1\nST92023\n\n\n2\n2022-01-01 02:00:00\n42.32\n-79.88\n0.140625\n2022-01-01 02:00:00\n1\nJan\n1\nST92023\n\n\n3\n2022-01-01 03:00:00\n42.32\n-79.88\n0.140625\n2022-01-01 03:00:00\n1\nJan\n1\nST92023\n\n\n4\n2022-01-01 04:00:00\n42.32\n-79.88\n0.132812\n2022-01-01 04:00:00\n1\nJan\n1\nST92023\n\n\n\n\n\n\n\nThis data is arranged by hours, need to group into months as well\n\nwave_month = wave.groupby([\"month_int\", \"station\", \"longitude\", \"latitude\"])[[\"waveHs\"]].mean().reset_index()\n\nChange to a geodataframe\n\nwave_month[\"geometry\"] = gpd.points_from_xy(\n   wave_month[\"longitude\"], wave_month[\"latitude\"]\n)\nwave_month_gdf = gpd.GeoDataFrame(\n    wave_month, geometry=\"geometry\", crs=\"EPSG:4326\"\n)\n\nThe final wave height geodataframe looks like this\n\nwave_month_gdf.head()\n\n\n\n\n\n\n\n\nmonth_int\nstation\nlongitude\nlatitude\nwaveHs\ngeometry\n\n\n\n\n0\n1\nST92001\n-79.04\n42.76\n1.102995\nPOINT (-79.04000 42.76000)\n\n\n1\n1\nST92002\n-79.08\n42.76\n1.166224\nPOINT (-79.08000 42.76000)\n\n\n2\n1\nST92003\n-79.12\n42.72\n1.220169\nPOINT (-79.12000 42.72000)\n\n\n3\n1\nST92004\n-79.16\n42.68\n1.260833\nPOINT (-79.16000 42.68000)\n\n\n4\n1\nST92005\n-79.20\n42.64\n1.242513\nPOINT (-79.20000 42.64000)\n\n\n\n\n\n\n\n\n\nStep 4: Match wave and wind data\nFirstly, the wind data need to form a “container” that can be used to capture wave data near each point. Here each wind point will form a 8km buffer\nNote: Need to change to crs 3857 to make the buffer\n\nbuffered_wind_month_gdf = wind_month_gdf.copy().to_crs(epsg=3857)\nbuffered_wind_month_gdf[\"geometry\"] = buffered_wind_month_gdf.buffer(8e3)\n\nHave a look at the buffer location. It covers all the area\n\nbuffered_wind_month_gdf.explore(column=\"wind_strength\", \n                 tiles=\"CartoDB positron\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nChange back to 4326 to make it the same as the crs for wave data\n\nbuffer4326 = buffered_wind_month_gdf.to_crs(epsg=4326)\n\nBecause the point location is the same for different months, we need to seperatly do the sjoin for each month and then combine the results\nA function can be helpful to handle these sjoins\n\ndef sjoin_different_month(wave, wind):\n    \"\"\"Do the sjoin between wave and wind for each month\"\"\"\n    joined = gpd.sjoin(wave, wind, predicate=\"within\", how=\"right\")\n    return joined\n\n\njoin_list = []\nfor i in range(12):\n    month = i+1\n    wave = wave_month_gdf.loc[wave_month_gdf['month_int']==month]\n    wind = buffer4326.loc[buffer4326['month_int']==month]\n    joined = sjoin_different_month(wave, wind)\n    join_list.append(joined)\n    \n\nCombine the list into one dataframe\n\nsjoin_all = pd.concat(join_list)\n\n\n\nStep 5: Regression models\nWhen there is no overlap, the station column will not have data. We will use all the rows with station value to form a machine learning model to analyze the relationship between wind strength and wave height\nFirstly, select all the rows with wave data to form the base for machine learning\n\nmodel_set = sjoin_all[sjoin_all['station'].notna()]\n\n\nmodel_set.head()\n\n\n\n\n\n\n\n\nindex_left\nmonth_int_left\nstation\nlongitude_left\nlatitude_left\nwaveHs\nmonth_int_right\nlongitude_right\nlatitude_right\nwind_strength\ngeometry\n\n\n\n\n18\n22.0\n1.0\nST92023\n-79.88\n42.32\n1.310742\n1\n-79.900002\n42.299999\n2.197481\nPOLYGON ((-79.82814 42.30000, -79.82848 42.294...\n\n\n18\n21.0\n1.0\nST92022\n-79.84\n42.32\n1.283843\n1\n-79.900002\n42.299999\n2.197481\nPOLYGON ((-79.82814 42.30000, -79.82848 42.294...\n\n\n33\n21.0\n1.0\nST92022\n-79.84\n42.32\n1.283843\n1\n-79.800003\n42.299999\n1.935837\nPOLYGON ((-79.72814 42.30000, -79.72848 42.294...\n\n\n33\n20.0\n1.0\nST92021\n-79.80\n42.32\n1.310535\n1\n-79.800003\n42.299999\n1.935837\nPOLYGON ((-79.72814 42.30000, -79.72848 42.294...\n\n\n34\n19.0\n1.0\nST92020\n-79.76\n42.36\n1.318989\n1\n-79.800003\n42.400002\n2.548905\nPOLYGON ((-79.72814 42.40000, -79.72848 42.394...\n\n\n\n\n\n\n\nBefore building the regression model, we can first visualize the relationship between wind strength and wave height through a scatter plot\n\nmodel_set.hvplot.scatter(\n    x=\"wind_strength\",\n    y=\"waveHs\",\n    width=700,\n    scale=0.2,\n    alpha=0.9,\n    color='#E07069',\n    title='Relationship between Wind Strength and Wave Height',\n    xlabel=\"Wind Strength (m/s)\",\n    ylabel=\"Wave Height (m)\"\n)\n\n\n\n\n\n  \n\n\n\n\nObservation: It’s a roughly linear relationship, so we can start with a linear model\nFirstly, the whole dataset will be seperated into train and test sets\n\n# use a 70/30% split\ntrain_set, test_set = train_test_split(model_set, test_size=0.3, random_state=42)\n\nThese are new DataFrame objects, with lengths determined by the split percentage:\n\nprint(\"size of full dataset = \", len(model_set))\nprint(\"size of training dataset = \", len(train_set))\nprint(\"size of test dataset = \", len(test_set))\n\nsize of full dataset =  356\nsize of training dataset =  249\nsize of test dataset =  107\n\n\n\nPart 1: Linear model\n\nmodel = LinearRegression()\n\n# Features\nX_train = train_set['wind_strength'].values\nX_train = X_train[:, np.newaxis]\n\nX_test = test_set['wind_strength'].values\nX_test = X_test[:, np.newaxis]\n\n# Labels\ny_train = train_set['waveHs'].values\ny_test = test_set['waveHs'].values\n\nScale the wind strength values\n\nscaler = StandardScaler()\n\n\n# Scale the training features\nX_train_scaled = scaler.fit_transform(X_train)\n\n# Scale the test features\nX_test_scaled = scaler.fit_transform(X_test)\n\nFit on the training set and evaluate on the test set\n\nmodel.fit(X_train_scaled, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nmodel.score(X_test_scaled, y_test)\n\n0.3573253503257996\n\n\nNote: A model with R-squared coefficient of 0.357 means only 35.7% of the variation in the dataset can be explained by the model, which is pretty bad\nVisualize linear prediction\n\n# The values we want to predict (ranging from our min to max wind strength)\nwh_pred = np.linspace(1.026, 4.651, 100)\n\n# Sklearn needs the second axis!\nX_pred = wh_pred[:, np.newaxis]\n\ny_pred = model.predict(X_pred)\n\n\nwith plt.style.context(\"fivethirtyeight\"):\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    # Plot the predicted values\n    ax.plot(X_pred, y_pred, label=\"Predicted values\", color=\"#E07069\")\n\n    # Training data\n    ax.scatter(\n        model_set['wind_strength'],\n        model_set['waveHs'],\n        label=\"Training data\",\n        s=100,\n        zorder=10,\n        color=\"#6989E0\",\n    )\n\n    ax.legend()\n    ax.set_xlabel(\"Wind Strength (m/s)\")\n    ax.set_ylabel(\"Wave Height (m)\")\n    plt.show()\n\n\n\n\nObservation:\nThe linear model will significantly overpredict the wave height.\n\n\nPart 2: Non-linear model\nSince the linear model does not work will, we can try to add new polynomial features from the wind strength data. Besides, we will turn our preprocessing steps into a Pipeline object to make multiple transformations easier\nNote: After multiple testings, the degree 2 model is the best, so the following steps will use a degree value of 2\n\npoly = PolynomialFeatures(degree=2)\n\n\n# Training\nX_train_scaled_poly = poly.fit_transform(scaler.fit_transform(X_train))\n\n# Test\nX_test_scaled_poly = poly.fit_transform(scaler.fit_transform(X_test))\n\n\nmodel.fit(X_train_scaled_poly, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nmodel.score(X_test_scaled_poly, y_test)\n\n0.35844964445219063\n\n\nApply the pipeline to predict the wave height in the test set\n\npipe = make_pipeline(StandardScaler(), PolynomialFeatures(degree=2))\n\npipe\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('polynomialfeatures', PolynomialFeatures())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('polynomialfeatures', PolynomialFeatures())])StandardScalerStandardScaler()PolynomialFeaturesPolynomialFeatures()\n\n\n\ny_pred = model.predict(pipe.fit_transform(X_pred))\n\n\nwith plt.style.context(\"fivethirtyeight\"):\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    # Plot the predicted values\n    y_pred = model.predict(pipe.fit_transform(X_pred))\n    ax.plot(X_pred, y_pred, label=\"Predicted values\", color=\"#E07069\")\n\n    # Training data\n    ax.scatter(\n        model_set['wind_strength'],\n        model_set['waveHs'],\n        label=\"Training data\",\n        s=100,\n        zorder=10,\n        color=\"#6989E0\",\n    )\n\n    ax.legend()\n    ax.set_xlabel(\"Wind Strength (m/s)\")\n    ax.set_ylabel(\"Wave Height (m)\")\n    plt.show()\n\n\n\n\nObservation:\nAlthough the non-linear model is slightly better than the linear one, it is still very bad. It is not in a good condition to predict the wave height by using wind strength\nConclusion: The wave height is a result of multiple factors. It is true that usually when wind strength increase, wave height will increase, this relationship doesn’t have a strong regression relationship. More factors are needed to predict the wave height. Besides, the inputting data is not detailed enough to capture the real condition along the shoreline. Finer data may also possibly improve the model."
  }
]