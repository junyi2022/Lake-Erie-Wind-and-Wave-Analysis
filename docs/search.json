[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lake Erie Wind and Wave Data Analysis and Visulization",
    "section": "",
    "text": "This project is a part of my data analysis for my research assistant work. It includes data gathering from multiple sources, visualizations, and regression models. In the analysis tab on the left, you can browse through the process and have a look at the final products for both wind and wave data. The final step includes a regression analysis that is built on all the previous steps.\n\n\n\nData is collected through a means more sophisticated than downloading (e.g. scraping, API).\nIt combines data collected from 3 or more different sources.\nThe analysis of the data is reasonably complex, involving multiple steps (geospatial joins/operations, data shaping, data frame operations, etc).\nYou analyze raster data using rasterio, rasterstats, or xarray.\nYou perform a machine learning analysis with scikit-learn as part of the analysis.\nThe project includes multiple interactive visualizations that include a significant interactive component (cross-filtering, interactive widgets, etc)"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Lake Erie Wind and Wave Data Analysis and Visulization",
    "section": "",
    "text": "This project is a part of my data analysis for my research assistant work. It includes data gathering from multiple sources, visualizations, and regression models. In the analysis tab on the left, you can browse through the process and have a look at the final products for both wind and wave data. The final step includes a regression analysis that is built on all the previous steps.\n\n\n\nData is collected through a means more sophisticated than downloading (e.g. scraping, API).\nIt combines data collected from 3 or more different sources.\nThe analysis of the data is reasonably complex, involving multiple steps (geospatial joins/operations, data shaping, data frame operations, etc).\nYou analyze raster data using rasterio, rasterstats, or xarray.\nYou perform a machine learning analysis with scikit-learn as part of the analysis.\nThe project includes multiple interactive visualizations that include a significant interactive component (cross-filtering, interactive widgets, etc)"
  },
  {
    "objectID": "index.html#find-out-more",
    "href": "index.html#find-out-more",
    "title": "Lake Erie Wind and Wave Data Analysis and Visulization",
    "section": "Find out more",
    "text": "Find out more\nThe code for this repository is hosted on GitHub page: https://github.com/junyi2022/musa-550-quarto."
  },
  {
    "objectID": "analysis/web-scraping-wave-data.html",
    "href": "analysis/web-scraping-wave-data.html",
    "title": "Web Scraping of Wave Height Data from Buoys",
    "section": "",
    "text": "Wave Data Source: WIS Data Portal\nThe wave data in this project comes from the buoys’ on Lake Erie. The data gathering process uses web scraping because there are too many bouys to be downloaded manully.\nThe time period will be 2022 yearly data, and the area of interest is the part of Lake Erie in New York State’s boundary (highlighted in yellow).\n\n\nimport requests\n# Import the webdriver from selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\n\n\nfrom time import sleep\n\n\nStep 1: Set up driver\n\ndriver = webdriver.Chrome()\n\nThis website takes a long time to load, so here we will stop for 10 seconds before continue the following steps\n\nurl = \"https://wisportal.erdc.dren.mil/#\"\ndriver.get(url)\nsleep(10)\n\n\n\nStep 2: Add wave height data of each buoy to an export group online\nBecause the buoys buttons on the website are markers generated by leaflet, the ID selectors associated with each marker are in a random numbering. Firstly, we need to have a list of all the buoy ID in the prefered sequence, which is from the south to the north.\n\n\n\n\n\n\nImportant\n\n\n\nBecause of the randomness associated with the leaflet marker ID, some of them may not be the same ID for each session (although most of them will stay the same). The ID may be slightly varied from the list below. Make sure to recheck them before reusing them.\n\n\n\nmarker_list = ['#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(3014)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2281)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2241)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2278)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2275)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2547)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2886)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2545)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2272)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2881)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(3009)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2269)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2266)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(3012)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(3010)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2263)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2876)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2872)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2543)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(3006)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2867)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(3007)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2541)',\n'#map-view &gt; div.leaflet-pane.leaflet-map-pane &gt; div.leaflet-pane.leaflet-marker-pane &gt; img:nth-child(2384)']\n\nSince there are a lot of buoys, a function can help us handle the scraping process of each individual\n\ndef add_each_marker_to_export_group(marker_selector):\n    \"\"\"handle the scraping process of each marker\"\"\"\n    # click marker\n    buoy_input = driver.find_element(By.CSS_SELECTOR, marker_selector)\n    buoy_input.click()\n    sleep(1)\n    \n    # click general export button\n    general_export_selector = \"#generic_export\"\n    general_export_input = driver.find_element(By.CSS_SELECTOR, general_export_selector)\n    general_export_input.click()\n    sleep(1)\n    \n    # check wave height checkbox\n    waveheight_selector = \"#check-waveHs\"\n    waveheight_check = driver.find_element(By.CSS_SELECTOR, waveheight_selector)\n    waveheight_check.click()\n    sleep(1)\n    \n    # add to export group\n    add_to_export_group_selector = \"#ep-export-button\"\n    add_to_export_group_button = driver.find_element(By.CSS_SELECTOR, add_to_export_group_selector)\n    add_to_export_group_button.click()\n    sleep(1)\n\nApply the above function to each marker in the list\n\nfor i in range(len(marker_list)):\n    marker_selector = marker_list[i]\n    add_each_marker_to_export_group(marker_selector)\n    sleep(1)\n    \n\n\n\nStep 3: Download the export group\nAfter adding all the buoy data we need, we will download the export group summary, which will be a zip file\n\n# Go to the export group summary page\nexport_group_selector = \"#export-summary\"\nexport_group_button = driver.find_element(By.CSS_SELECTOR, export_group_selector)\nexport_group_button.click()\n\n# Download the data we need\ndownload_selector = \"#ep-download-all-button\"\ndownload_button = driver.find_element(By.CSS_SELECTOR, download_selector)\ndownload_button.click()\nsleep(10)\n\n\n\nStep 4: Close Driver\n\ndriver.close()"
  },
  {
    "objectID": "analysis/wave-visualization.html",
    "href": "analysis/wave-visualization.html",
    "title": "Wave Data Visualization",
    "section": "",
    "text": "Wave Data Source: WIS Data Portal\nThe wave data in this project comes from the web scraping page.\nThe time period will be 2022 yearly data, and the area of interest is the part of Lake Erie in New York State’s boundary.\n\nimport altair as alt\nimport numpy as np\nimport pandas as pd\nimport holoviews as hv\nimport hvplot.pandas\n# Load bokeh\nhv.extension(\"bokeh\")\nimport geopandas as gpd\n%matplotlib inline\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport panel as pn\n\n\nStep 1: Read all the downloaded data from csv\nThe zip file from the web scraping process includes multiple csv (one for each buoy)\n\nST92023_raw = pd.read_csv(\"./data/web-scarp/1-ST92023-generic_export-20231218T20_22.csv\")\nST92022_raw = pd.read_csv(\"./data/web-scarp/2-ST92022-generic_export-20231218T20_22.csv\")\nST92021_raw = pd.read_csv(\"./data/web-scarp/3-ST92021-generic_export-20231218T20_22.csv\")\nST92020_raw = pd.read_csv(\"./data/web-scarp/4-ST92020-generic_export-20231218T20_22.csv\")\nST92019_raw = pd.read_csv(\"./data/web-scarp/5-ST92019-generic_export-20231218T20_22.csv\")\nST92018_raw = pd.read_csv(\"./data/web-scarp/6-ST92018-generic_export-20231218T20_22.csv\")\nST92017_raw = pd.read_csv(\"./data/web-scarp/7-ST92017-generic_export-20231218T20_22.csv\")\nST92016_raw = pd.read_csv(\"./data/web-scarp/8-ST92016-generic_export-20231218T20_22.csv\")\nST92015_raw = pd.read_csv(\"./data/web-scarp/9-ST92015-generic_export-20231218T20_22.csv\")\nST92014_raw = pd.read_csv(\"./data/web-scarp/10-ST92014-generic_export-20231218T20_22.csv\")\nST92013_raw = pd.read_csv(\"./data/web-scarp/11-ST92013-generic_export-20231218T20_22.csv\")\nST92012_raw = pd.read_csv(\"./data/web-scarp/12-ST92012-generic_export-20231218T20_22.csv\")\nST92011_raw = pd.read_csv(\"./data/web-scarp/13-ST92011-generic_export-20231218T20_22.csv\")\nST92010_raw = pd.read_csv(\"./data/web-scarp/14-ST92010-generic_export-20231218T20_22.csv\")\nST92009_raw = pd.read_csv(\"./data/web-scarp/15-ST92009-generic_export-20231218T20_22.csv\")\nST92008_raw = pd.read_csv(\"./data/web-scarp/16-ST92008-generic_export-20231218T20_22.csv\")\nST92007_raw = pd.read_csv(\"./data/web-scarp/17-ST92007-generic_export-20231218T20_22.csv\")\nST92006_raw = pd.read_csv(\"./data/web-scarp/18-ST92006-generic_export-20231218T20_22.csv\")\nST92005_raw = pd.read_csv(\"./data/web-scarp/19-ST92005-generic_export-20231218T20_22.csv\")\nST92004_raw = pd.read_csv(\"./data/web-scarp/20-ST92004-generic_export-20231218T20_22.csv\")\nST92003_raw = pd.read_csv(\"./data/web-scarp/21-ST92003-generic_export-20231218T20_22.csv\")\nST92002_raw = pd.read_csv(\"./data/web-scarp/22-ST92002-generic_export-20231218T20_22.csv\")\nST92001_raw = pd.read_csv(\"./data/web-scarp/23-ST92001-generic_export-20231218T20_22.csv\")\nST92243_raw = pd.read_csv(\"./data/web-scarp/24-ST92243-generic_export-20231218T20_22.csv\")\n\n\n# A view of the raw data:\nST92023_raw.head()\n\n\n\n\n\n\n\n\ntime\nlatitude\nlongitude\nwaveHs\n\n\n\n\n0\n2022-01-01 00:00:00\n42.32\n-79.88\n0.132812\n\n\n1\n2022-01-01 01:00:00\n42.32\n-79.88\n0.140625\n\n\n2\n2022-01-01 02:00:00\n42.32\n-79.88\n0.140625\n\n\n3\n2022-01-01 03:00:00\n42.32\n-79.88\n0.140625\n\n\n4\n2022-01-01 04:00:00\n42.32\n-79.88\n0.132812\n\n\n\n\n\n\n\n\n# Date type for each column\nST92023_raw.dtypes\n\ntime          object\nlatitude     float64\nlongitude    float64\nwaveHs       float64\ndtype: object\n\n\n\n\nStep 2: Clean the raw data\nSince there are 24 files, a list will be helpful for later manipulations\n\nbuoy_list_raw = [\nST92023_raw, ST92022_raw, ST92021_raw, ST92020_raw, ST92019_raw,\nST92018_raw, ST92017_raw, ST92016_raw, ST92015_raw, ST92014_raw,\nST92013_raw, ST92012_raw, ST92011_raw, ST92010_raw, ST92009_raw,\nST92008_raw, ST92007_raw, ST92006_raw, ST92005_raw, ST92004_raw,\nST92003_raw, ST92002_raw, ST92001_raw, ST92243_raw\n]\n\nFirstly, clean the raw data. This process includes handling datetime object and drop null value.\n\nbuoy_list = []\nfor buoy in buoy_list_raw:\n    #Convert date type\n    buoy[\"datetime\"] = pd.to_datetime(buoy[\"time\"])\n    \n    #Drop Null value\n    buoy_clean = buoy.drop(buoy[buoy['waveHs'] &lt; 0].index)\n    \n    # Save it\n    buoy_list.append(buoy_clean)\n\n\nlen(buoy_list)\n\n24\n\n\nAnd then, change each buoy’s data to a prefered structure, which includes a column for month and a column for day of year. Each buoy will also have its station name in the column\n\nbuoy_name = [\n'ST92023', 'ST92022', 'ST92021', 'ST92020', 'ST92019',\n'ST92018', 'ST92017', 'ST92016', 'ST92015', 'ST92014',\n'ST92013', 'ST92012', 'ST92011', 'ST92010', 'ST92009',\n'ST92008', 'ST92007', 'ST92006', 'ST92005', 'ST92004',\n'ST92003', 'ST92002', 'ST92001', 'ST92243'\n]\n\n\nfor i in range(len(buoy_list)):\n    buoy = buoy_list[i]\n    \n    #Arrange by month prepare\n    buoy[\"month_int\"] = buoy[\"datetime\"].dt.month\n    buoy[\"month\"] = buoy[\"datetime\"].dt.strftime(\"%b\")\n    \n    #Arrange by date prepare\n    buoy[\"day_int\"] = buoy[\"datetime\"].dt.dayofyear\n    \n    # Add station name\n    buoy[\"station\"] = buoy_name[i]\n    \n\nComebine seperate dataframes into a single dataframe\n\nbuoy_clean_combine = pd.concat(buoy_list)\n\n\nlen(buoy_clean_combine)\n\n187680\n\n\nA view of the final dataframe\n\nbuoy_clean_combine.head()\n\n\n\n\n\n\n\n\ntime\nlatitude\nlongitude\nwaveHs\ndatetime\nmonth_int\nmonth\nday_int\nstation\n\n\n\n\n0\n2022-01-01 00:00:00\n42.32\n-79.88\n0.132812\n2022-01-01 00:00:00\n1\nJan\n1\nST92023\n\n\n1\n2022-01-01 01:00:00\n42.32\n-79.88\n0.140625\n2022-01-01 01:00:00\n1\nJan\n1\nST92023\n\n\n2\n2022-01-01 02:00:00\n42.32\n-79.88\n0.140625\n2022-01-01 02:00:00\n1\nJan\n1\nST92023\n\n\n3\n2022-01-01 03:00:00\n42.32\n-79.88\n0.140625\n2022-01-01 03:00:00\n1\nJan\n1\nST92023\n\n\n4\n2022-01-01 04:00:00\n42.32\n-79.88\n0.132812\n2022-01-01 04:00:00\n1\nJan\n1\nST92023\n\n\n\n\n\n\n\nSave the dataframe for future usage\n\nbuoy_clean_combine.to_csv(\"./data/wave2022.csv\", index=False)\n\n\n\nStep 3: Line Chart Visualization\nThe first line chart plot shows the wave height of different buoys in 2022. By dragging the widget from left to right, you can have a look at the buoys located from south to north\n\nclean_combine_chart = buoy_clean_combine.hvplot(\n    x=\"datetime\", \n    y=\"waveHs\", \n    groupby=\"station\", \n    width=900, \n    kind=\"line\", \n    widgets={'station': pn.widgets.DiscreteSlider},\n    widget_location='bottom',\n    color='#E07069',\n    line_width=0.8,\n    title='Lake Erie NYS Shoreline 2022 Wave Height',\n    xlabel=\"Date\",\n    ylabel=\"Wave Height / m\"\n)\n\nclean_combine_chart\n\n\n\n\n\n\n\n\n  \n\n\n\n\nObservations:\nThe wave height recorded by each buoy is usually turbulent between different days. However, the overall trend is that buoys encounter higher waves in winter.\nNote:\nAfter searching for hours I would conclude that Quarto does not support the hvplot widget in the notebook. If want to see the live version of the above widget, please refer to final project’s repository and open it in jupyter notebook instead. There are discussions about this issue. The conclusion is that “The kind of Jupyter interactivity that we can’t easily support directly in Quarto is that which involves a live Jupyter process for every open web browser. In order for ipywidgets.interact to work, a jupyter kernel needs to be live and running, and that is a fundamentally more involved mode of deployment.” Alternatively, there are ways to possibly solve this issue by using shinylive package which involves NodeJS and suported by Quarto website. However, NodeJS is out of the scope of this project and will not be included here.\nThe second line plot compares the southest buoy to the northest buoy in order to show the trend across the shoreline. ST92023 is the southest buoy along the NYS shoreline of Lake Erie, and ST92243 is the northest buoy along the NYS shoreline of Lake Erie, near Buffalo\n\ntwo_end = ['ST92243', 'ST92023']\nends = buoy_clean_combine.loc[buoy_clean_combine['station'].isin(two_end)]\n\n\nend_chart = ends.hvplot(\n    x=\"datetime\", \n    y=\"waveHs\", \n    by=\"station\",\n    kind=\"line\",\n    color=['#E07069', '#6989E0'],\n    line_width=0.8,\n    width=900,\n    title=\"Compare North and South End's Wave Height\",\n    xlabel=\"Date\",\n    ylabel=\"Wave Height / m\"\n)\nend_chart\n\n\n\n\n\n  \n\n\n\n\nObservation:\nThe south buoy has higher waves than the north buoy. This is probably because the prevailing wind of Lake Erie is southwest. Besides, the north buoy has a long period without data (from February to April), which is probably because the lake is frozen during that time.\n\n\nStep 4: Heat map visua\nThis step will calculate the average wave height for each month or each day in 2022 and show the results in heatmaps\n\nheight_heat = buoy_clean_combine.hvplot.heatmap(x='month_int', \n                                                y='station', \n                                                C='waveHs', \n                                                reduce_function=np.mean, \n                                                cmap='Magma', \n                                                width=900, \n                                                height=300, \n                                                colorbar=True,\n                                                title=\"Lake Erie NYS Shoreline 2022 Buoys'Wave Height by Month\")\n\nheight_heat.redim(station=\"Stations\", month_int=\"Month\")\n\n\n\n\n\n  \n\n\n\n\nObservation:\nThe wave height is higher in winter (November to January) and lower in summer (May to August). There is a small time lag for the change of wave height, which means wave height of south shoreline changes earlier than the north shoreline.\n\nheight_heat = buoy_clean_combine.hvplot.heatmap(x='day_int', \n                                                y='station', \n                                                C='waveHs', \n                                                reduce_function=np.mean, \n                                                width=900, \n                                                height=300, \n                                                colorbar=True,\n                                                cmap='Magma',\n                                                title=\"Lake Erie NYS Shoreline 2022 Buoys'Wave Height by Day\")\nheight_heat.redim(station=\"Stations\", day_int=\"Day of 2022\")\n\n\n\n\n\n  \n\n\n\n\nObservation:\nIn Febuary and March, a lot of buoys have missing data, which is very likly because of the frozen lake. In December, there was a large blizzards and seiche event, which is indicated in this plot. In a single day, the wave height was above 7 meters for almost all the buoys along the NYS shoreline of Lake Erie."
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nThis section includes technical analysis done using Jupyter notebooks. Each sub-section highlights different components of analysis and visualizations in this project.\nYou can also see those as steps to processing the data.\n1. Wind data cleaning and visualizations\n2. Web scraping to gather wave data\n3. Wave data cleaning and visualizations\n4. Wind data and wave data correlation analysis using regression models"
  },
  {
    "objectID": "analysis/wave-wind-analysis.html",
    "href": "analysis/wave-wind-analysis.html",
    "title": "Wind Strength and Wave Height Correlation Analysis",
    "section": "",
    "text": "Data Source:\n1. Wind data comes from ERA5 climate data from Climate Data Store The wind data in this project comes from the ‘ERA5 monthly averaged data on single levels from 1940 to present’ dataset, specifically the 10m u-component of wind and 10m v-component of wind.\nThe time period will be 2022 yearly data, and the area of interest will be Lake Erie. 2. Wave data comes from the final dataframe of the wave visualization page.\n\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport xarray as xr\nfrom matplotlib import pyplot as plt\nimport holoviews as hv\nimport hvplot.pandas\n# Load bokeh\nhv.extension(\"bokeh\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\n\nStep 1: Read the .nc file downloaded from the ERA5 website and change it to a dataframe\nNeed to use xarry to read netCDF file\n\nwindnc = xr.open_dataset('./data/ERA5land.nc') \nwinddf = windnc.to_dataframe().reset_index()\n\nThe raw data looks like this\n\nwinddf.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\ntime\nu10\nv10\n\n\n\n\n0\n-80.0\n43.400002\n2022-01-01\n1.490095\n-0.002929\n\n\n1\n-80.0\n43.400002\n2022-02-01\n1.628315\n0.594098\n\n\n2\n-80.0\n43.400002\n2022-03-01\n1.543347\n-0.034178\n\n\n3\n-80.0\n43.400002\n2022-04-01\n1.104076\n0.152090\n\n\n4\n-80.0\n43.400002\n2022-05-01\n-0.122249\n0.055608\n\n\n\n\n\n\n\n\nwinddf.dtypes\n\nlongitude           float64\nlatitude            float64\ntime         datetime64[ns]\nu10                 float32\nv10                 float32\ndtype: object\n\n\n\n\nStep 2: Calculate wind strength and change to geodataframe\nThe wind strength is based on calculation of wind u and wind v\n\nwinddf['wind_strength'] = np.sqrt(winddf['u10']**2 + winddf['v10']**2)\n\nCurrently the data is arranged by days, but we need month value. Manipulate the dataframe to get the month column\n\nwinddf[\"month_int\"] = winddf[\"time\"].dt.month\nwind_month = winddf.groupby([\"month_int\", \"longitude\", \"latitude\"])[[\"wind_strength\"]].mean().reset_index()\n\nUse the latitude and longitude column to get a geodataframe of wind data\n\nwind_month[\"geometry\"] = gpd.points_from_xy(\n   wind_month[\"longitude\"], wind_month[\"latitude\"]\n)\nwind_month_gdf = gpd.GeoDataFrame(\n    wind_month, geometry=\"geometry\", crs=\"EPSG:4326\"\n)\n\nThe geodataframe looks like this\n\nwind_month_gdf.head()\n\n\n\n\n\n\n\n\nmonth_int\nlongitude\nlatitude\nwind_strength\ngeometry\n\n\n\n\n0\n1\n-80.0\n42.000000\n1.075989\nPOINT (-80.00000 42.00000)\n\n\n1\n1\n-80.0\n42.099998\n1.489239\nPOINT (-80.00000 42.10000)\n\n\n2\n1\n-80.0\n42.200001\n1.999220\nPOINT (-80.00000 42.20000)\n\n\n3\n1\n-80.0\n42.299999\n2.441840\nPOINT (-80.00000 42.30000)\n\n\n4\n1\n-80.0\n42.400002\n2.715811\nPOINT (-80.00000 42.40000)\n\n\n\n\n\n\n\nHave a look at the location of each wind data’s point\n\nwind_month_gdf.explore(column=\"wind_strength\", \n                 tiles=\"CartoDB positron\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nStep 3: Read the wave dataframe that is cleaned in the wave data visualization page\n\nwave = pd.read_csv(\"./data/wave2022.csv\")\n\n\nwave.head()\n\n\n\n\n\n\n\n\ntime\nlatitude\nlongitude\nwaveHs\ndatetime\nmonth_int\nmonth\nday_int\nstation\n\n\n\n\n0\n2022-01-01 00:00:00\n42.32\n-79.88\n0.132812\n2022-01-01 00:00:00\n1\nJan\n1\nST92023\n\n\n1\n2022-01-01 01:00:00\n42.32\n-79.88\n0.140625\n2022-01-01 01:00:00\n1\nJan\n1\nST92023\n\n\n2\n2022-01-01 02:00:00\n42.32\n-79.88\n0.140625\n2022-01-01 02:00:00\n1\nJan\n1\nST92023\n\n\n3\n2022-01-01 03:00:00\n42.32\n-79.88\n0.140625\n2022-01-01 03:00:00\n1\nJan\n1\nST92023\n\n\n4\n2022-01-01 04:00:00\n42.32\n-79.88\n0.132812\n2022-01-01 04:00:00\n1\nJan\n1\nST92023\n\n\n\n\n\n\n\nThis data is arranged by hours, need to group into months as well\n\nwave_month = wave.groupby([\"month_int\", \"station\", \"longitude\", \"latitude\"])[[\"waveHs\"]].mean().reset_index()\n\nChange to a geodataframe\n\nwave_month[\"geometry\"] = gpd.points_from_xy(\n   wave_month[\"longitude\"], wave_month[\"latitude\"]\n)\nwave_month_gdf = gpd.GeoDataFrame(\n    wave_month, geometry=\"geometry\", crs=\"EPSG:4326\"\n)\n\nThe final wave height geodataframe looks like this\n\nwave_month_gdf.head()\n\n\n\n\n\n\n\n\nmonth_int\nstation\nlongitude\nlatitude\nwaveHs\ngeometry\n\n\n\n\n0\n1\nST92001\n-79.04\n42.76\n1.102995\nPOINT (-79.04000 42.76000)\n\n\n1\n1\nST92002\n-79.08\n42.76\n1.166224\nPOINT (-79.08000 42.76000)\n\n\n2\n1\nST92003\n-79.12\n42.72\n1.220169\nPOINT (-79.12000 42.72000)\n\n\n3\n1\nST92004\n-79.16\n42.68\n1.260833\nPOINT (-79.16000 42.68000)\n\n\n4\n1\nST92005\n-79.20\n42.64\n1.242513\nPOINT (-79.20000 42.64000)\n\n\n\n\n\n\n\n\n\nStep 4: Match wave and wind data\nFirstly, the wind data need to form a “container” that can be used to capture wave data near each point. Here each wind point will form a 8km buffer\nNote: Need to change to crs 3857 to make the buffer\n\nbuffered_wind_month_gdf = wind_month_gdf.copy().to_crs(epsg=3857)\nbuffered_wind_month_gdf[\"geometry\"] = buffered_wind_month_gdf.buffer(8e3)\n\nHave a look at the buffer location. It covers all the area\n\nbuffered_wind_month_gdf.explore(column=\"wind_strength\", \n                 tiles=\"CartoDB positron\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nChange back to 4326 to make it the same as the crs for wave data\n\nbuffer4326 = buffered_wind_month_gdf.to_crs(epsg=4326)\n\nBecause the point location is the same for different months, we need to seperatly do the sjoin for each month and then combine the results\nA function can be helpful to handle these sjoins\n\ndef sjoin_different_month(wave, wind):\n    \"\"\"Do the sjoin between wave and wind for each month\"\"\"\n    joined = gpd.sjoin(wave, wind, predicate=\"within\", how=\"right\")\n    return joined\n\n\njoin_list = []\nfor i in range(12):\n    month = i+1\n    wave = wave_month_gdf.loc[wave_month_gdf['month_int']==month]\n    wind = buffer4326.loc[buffer4326['month_int']==month]\n    joined = sjoin_different_month(wave, wind)\n    join_list.append(joined)\n    \n\nCombine the list into one dataframe\n\nsjoin_all = pd.concat(join_list)\n\n\n\nStep 5: Regression models\nWhen there is no overlap, the station column will not have data. We will use all the rows with station value to form a machine learning model to analyze the relationship between wind strength and wave height\nFirstly, select all the rows with wave data to form the base for machine learning\n\nmodel_set = sjoin_all[sjoin_all['station'].notna()]\n\n\nmodel_set.head()\n\n\n\n\n\n\n\n\nindex_left\nmonth_int_left\nstation\nlongitude_left\nlatitude_left\nwaveHs\nmonth_int_right\nlongitude_right\nlatitude_right\nwind_strength\ngeometry\n\n\n\n\n18\n22.0\n1.0\nST92023\n-79.88\n42.32\n1.310742\n1\n-79.900002\n42.299999\n2.197481\nPOLYGON ((-79.82814 42.30000, -79.82848 42.294...\n\n\n18\n21.0\n1.0\nST92022\n-79.84\n42.32\n1.283843\n1\n-79.900002\n42.299999\n2.197481\nPOLYGON ((-79.82814 42.30000, -79.82848 42.294...\n\n\n33\n21.0\n1.0\nST92022\n-79.84\n42.32\n1.283843\n1\n-79.800003\n42.299999\n1.935837\nPOLYGON ((-79.72814 42.30000, -79.72848 42.294...\n\n\n33\n20.0\n1.0\nST92021\n-79.80\n42.32\n1.310535\n1\n-79.800003\n42.299999\n1.935837\nPOLYGON ((-79.72814 42.30000, -79.72848 42.294...\n\n\n34\n19.0\n1.0\nST92020\n-79.76\n42.36\n1.318989\n1\n-79.800003\n42.400002\n2.548905\nPOLYGON ((-79.72814 42.40000, -79.72848 42.394...\n\n\n\n\n\n\n\nBefore building the regression model, we can first visualize the relationship between wind strength and wave height through a scatter plot\n\nmodel_set.hvplot.scatter(\n    x=\"wind_strength\",\n    y=\"waveHs\",\n    width=700,\n    scale=0.2,\n    alpha=0.9,\n    color='#E07069',\n    title='Relationship between Wind Strength and Wave Height',\n    xlabel=\"Wind Strength (m/s)\",\n    ylabel=\"Wave Height (m)\"\n)\n\n\n\n\n\n  \n\n\n\n\nObservation: It’s a roughly linear relationship, so we can start with a linear model\nFirstly, the whole dataset will be seperated into train and test sets\n\n# use a 70/30% split\ntrain_set, test_set = train_test_split(model_set, test_size=0.3, random_state=42)\n\nThese are new DataFrame objects, with lengths determined by the split percentage:\n\nprint(\"size of full dataset = \", len(model_set))\nprint(\"size of training dataset = \", len(train_set))\nprint(\"size of test dataset = \", len(test_set))\n\nsize of full dataset =  356\nsize of training dataset =  249\nsize of test dataset =  107\n\n\n\nPart 1: Linear model\n\nmodel = LinearRegression()\n\n# Features\nX_train = train_set['wind_strength'].values\nX_train = X_train[:, np.newaxis]\n\nX_test = test_set['wind_strength'].values\nX_test = X_test[:, np.newaxis]\n\n# Labels\ny_train = train_set['waveHs'].values\ny_test = test_set['waveHs'].values\n\nScale the wind strength values\n\nscaler = StandardScaler()\n\n\n# Scale the training features\nX_train_scaled = scaler.fit_transform(X_train)\n\n# Scale the test features\nX_test_scaled = scaler.fit_transform(X_test)\n\nFit on the training set and evaluate on the test set\n\nmodel.fit(X_train_scaled, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nmodel.score(X_test_scaled, y_test)\n\n0.3573253503257996\n\n\nNote: A model with R-squared coefficient of 0.357 means only 35.7% of the variation in the dataset can be explained by the model, which is pretty bad\nVisualize linear prediction\n\n# The values we want to predict (ranging from our min to max wind strength)\nwh_pred = np.linspace(1.026, 4.651, 100)\n\n# Sklearn needs the second axis!\nX_pred = wh_pred[:, np.newaxis]\n\ny_pred = model.predict(X_pred)\n\n\nwith plt.style.context(\"fivethirtyeight\"):\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    # Plot the predicted values\n    ax.plot(X_pred, y_pred, label=\"Predicted values\", color=\"#E07069\")\n\n    # Training data\n    ax.scatter(\n        model_set['wind_strength'],\n        model_set['waveHs'],\n        label=\"Training data\",\n        s=100,\n        zorder=10,\n        color=\"#6989E0\",\n    )\n\n    ax.legend()\n    ax.set_xlabel(\"Wind Strength (m/s)\")\n    ax.set_ylabel(\"Wave Height (m)\")\n    plt.show()\n\n\n\n\nObservation:\nThe linear model will significantly overpredict the wave height.\n\n\nPart 2: Non-linear model\nSince the linear model does not work will, we can try to add new polynomial features from the wind strength data. Besides, we will turn our preprocessing steps into a Pipeline object to make multiple transformations easier\nNote: After multiple testings, the degree 2 model is the best, so the following steps will use a degree value of 2\n\npoly = PolynomialFeatures(degree=2)\n\n\n# Training\nX_train_scaled_poly = poly.fit_transform(scaler.fit_transform(X_train))\n\n# Test\nX_test_scaled_poly = poly.fit_transform(scaler.fit_transform(X_test))\n\n\nmodel.fit(X_train_scaled_poly, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nmodel.score(X_test_scaled_poly, y_test)\n\n0.35844964445219063\n\n\nApply the pipeline to predict the wave height in the test set\n\npipe = make_pipeline(StandardScaler(), PolynomialFeatures(degree=2))\n\npipe\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('polynomialfeatures', PolynomialFeatures())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('polynomialfeatures', PolynomialFeatures())])StandardScalerStandardScaler()PolynomialFeaturesPolynomialFeatures()\n\n\n\ny_pred = model.predict(pipe.fit_transform(X_pred))\n\n\nwith plt.style.context(\"fivethirtyeight\"):\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    # Plot the predicted values\n    y_pred = model.predict(pipe.fit_transform(X_pred))\n    ax.plot(X_pred, y_pred, label=\"Predicted values\", color=\"#E07069\")\n\n    # Training data\n    ax.scatter(\n        model_set['wind_strength'],\n        model_set['waveHs'],\n        label=\"Training data\",\n        s=100,\n        zorder=10,\n        color=\"#6989E0\",\n    )\n\n    ax.legend()\n    ax.set_xlabel(\"Wind Strength (m/s)\")\n    ax.set_ylabel(\"Wave Height (m)\")\n    plt.show()\n\n\n\n\nObservation:\nAlthough the non-linear model is slightly better than the linear one, it is still very bad. It is not in a good condition to predict the wave height by using wind strength\nConclusion: The wave height is a result of multiple factors. It is true that usually when wind strength increase, wave height will increase, this relationship doesn’t have a strong regression relationship. More factors are needed to predict the wave height. Besides, the inputting data is not detailed enough to capture the real condition along the shoreline. Finer data may also possibly improve the model."
  },
  {
    "objectID": "analysis/wind-data-visulize.html",
    "href": "analysis/wind-data-visulize.html",
    "title": "Wind Data Visualization",
    "section": "",
    "text": "Wind Data Source: ERA5 climate data from Climate Data Store\nThe wind data in this project comes from the ‘ERA5 monthly averaged data on single levels from 1940 to present’ dataset, specifically the 10m u-component of wind and 10m v-component of wind.\nThe time period will be 2022 yearly data, and the area of interest will be Lake Erie.\n\n\n\n\n\n\nNote\n\n\n\nEven though during downloading process, it says the data is in a 10 meters resolution, the actual resolution of the .nc file is much larger than that.\n\n\n\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport xarray as xr\nfrom matplotlib import pyplot as plt\nimport datashader as ds\nimport imageio\n\n\nStep 1: Read the .nc file downloaded from the ERA5 website and calculate wind strength\nRead .nc file downloaded from Climate Data Store\n\nwindnc = xr.open_dataset('./data/ERA5climate4.nc')  \n\nExtract latitude and longitude data of each point\n\nlat = windnc.latitude\nlon = windnc.longitude\n\nExtract wind uv data\n\nwind_u = windnc.u10\nwind_v = windnc.v10\n\nCalculate wind strength (m/s) based on wind uv data\n\nWS = np.sqrt(wind_u**2 + wind_v**2)\n\n\n\nStep 2: plot wind field map for each month in 2022 and generate a gif\nRead the Lake Erie shapefile extracted from USGS National Hydrography Dataset and change it to the same projection as wind data (CRS 4326)\n\nlake = gpd.read_file(\"./data/LakeErie\")\n\n\nlake4326 = lake.to_crs(epsg=4326)\n\nDefine a function to collect the plot for each month\nThe plot contains three components: the wind field map (array array), the wind strength map (color), and Lake Erie location (shape).\nThe wind field map is generated through vector calculation.\nThe wind strength map is generated through each point’s relationship to its neighbors.\n\n# Have a list of month names for the function to use\nmonthlist = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n\n\ndef plot_wind_by_month(month):\n    \"\"\"Plot wind field map for each month in 2022\"\"\"\n    fig, ax = plt.subplots(figsize=(10,6))\n\n    # Wind strength map\n    cf = plt.contourf(lon,lat,WS[month,:,:], levels=15, cmap='rainbow', alpha=0.8)\n    \n    # lake Erie shape\n    lake4326.plot(ax=plt.gca(), edgecolor='#25408F', facecolor='none', linewidth=0.5)\n    \n    # Wind field map with labels\n    Q = plt.quiver(lon,lat,wind_u[month,:,:],wind_v[month,:,:], scale_units='xy', scale=4, width=0.0014, color='white')\n    qk = plt.quiverkey(Q, \n                  1, 1.02, \n                   1,str(1)+' m/s',   \n                   labelpos='E',                \n                   coordinates='axes',\n                   color='black'\n                   )\n\n    # style adjustments\n    cb = plt.colorbar(cf, fraction=0.0235, pad=0.03)\n    cb.set_label('m/s',  fontsize=11)\n    plt.xlabel('Longitude')\n    plt.ylabel('Latitude')\n    plt.title(f'Wind Field Map of {monthlist[month]}')\n\n    # prepare for imageio\n    fig.canvas.draw()\n    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype=\"uint8\")\n    image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n    \n    return image\n\nUse the function above to see the wind strenghth for each month and generate a gif for the wind visualization\n\nimgs = []\nfor month in range(12):\n    img = plot_wind_by_month(month)\n    imgs.append(img)\n\n# Combing the images for each month into a single GIF\nimageio.mimsave(\"wind-field.gif\", imgs, duration=1000)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe gif generated above does not have infinite loop, so Onlinegiftools can help change that property\nThe final visualization is below:\n\n\n\nStep 3: Analyze the result\nResult:\nThe gif works smoothly and clearly displays the wind change within 2022. The prevailing wind for Lake Erie is southwest, which is because of the typical seiche pattern. Because all the vectors of the wind field map are on the same scale, we can see that Lake Erie has severely larger wind in the winter.\nEvent:\nThe dominant southwest wind usually causes much damage to residents near the coastline in winter. Historic winter storms dumped 50 inches of snow per year in the region. In December 2022, blizzards and seiche killed 50 people, including 27 in Buffalo, and created a catastrophic blanket of ice across the city.\nCompare:\nThe gif result can reflect the event above. In November and December, the wind field arrays are much longer than the arrays in other months."
  }
]